{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data = sc.parallelize([\n",
    "      (1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD'\n",
    "        , 13.75, 9.48, 0.61, 4.02)\n",
    "    , (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD'\n",
    "        , 11.04, 7.74, 0.52, 2.03)\n",
    "    , (3, 'MacBook Air', 2016, '13.3\"', '8GB', '128GB SSD'\n",
    "        , 12.8, 8.94, 0.68, 2.96)\n",
    "    , (4, 'iMac', 2017, '27\"', '64GB', '1TB SSD'\n",
    "        , 25.6, 8.0, 20.3, 20.8)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data_df = spark.createDataFrame(sample_data,[\n",
    "        'Id'\n",
    "        , 'Model'\n",
    "        , 'Year'\n",
    "        , 'ScreenSize'\n",
    "        , 'RAM'\n",
    "        , 'HDD'\n",
    "        , 'W'\n",
    "        , 'D'\n",
    "        , 'H'\n",
    "        , 'Weight'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, Model=u'MacBook Pro', Year=2015, ScreenSize=u'15\"', RAM=u'16GB', HDD=u'512GB SSD', W=13.75, D=9.48, H=0.61, Weight=4.02)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-2f51c4013b4e>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-2f51c4013b4e>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    ,HDD_size=row.HDD.split(' ')[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "sample_data_transformed = (\n",
    "    sample_data_df\n",
    "    .rdd #Each element of the RDD inside the data frame is a ROW( ) object \n",
    "    .map(lambda row: sql.Row(\n",
    "        **row.asDict() \n",
    "        ,HDD_size=row.HDD.split(' ')[0]\n",
    "        )\n",
    "    )\n",
    "    .map(lambda row: sql.Row(\n",
    "        **row.asDict()\n",
    "        , HDD_type=row.HDD.split(' ')[1]\n",
    "        )\n",
    "    )\n",
    "    .map(lambda row: sql.Row(\n",
    "        **row.asDict()\n",
    "        , Volume=row.H * row.D * row.W\n",
    "        )\n",
    "    )\n",
    "    .toDF()\n",
    "    .select(\n",
    "        sample_data_df.columns + \n",
    "        [\n",
    "              'HDD_size'\n",
    "            , 'HDD_type'\n",
    "            , f.round(\n",
    "                f.col('Volume')\n",
    "            ).alias('Volume_cuIn')\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, Model=u'MacBook Pro', Year=2015, ScreenSize=u'15\"', RAM=u'16GB', HDD=u'512GB SSD', W=13.75, D=9.48, H=0.61, Weight=4.02)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_df.rdd.take(1) #map(lambda row : row[1]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD', 13.75, 9.48, 0.61, 4.02),\n",
       " (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD', 11.04, 7.74, 0.52, 2.03)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_data_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-84c0c7687bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_data_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_data_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "sample_data_transformed.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'asDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4285619f91cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'asDict'"
     ]
    }
   ],
   "source": [
    "sample_data_df.rdd.asDict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"create\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(0,'a'),(0,'b'),(0,'v'),(0,'j')],['id','letter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  0|     a|\n",
      "+---+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, letter=u'a')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)   # press Shift tab to get the options for the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|letter|newage|\n",
      "+---+------+------+\n",
      "|  0|     a|     0|\n",
      "|  0|     b|     0|\n",
      "|  0|     v|     0|\n",
      "|  0|     j|     0|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('newage',df['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc1 = SparkSession.builder.appName(\"Data_Frames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sc1.createDataFrame([(0,'a'),(1,'b'),(2,'c')],['id','letter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  0|     a|\n",
      "|  1|     b|\n",
      "|  2|     c|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/RahulReddy/Desktop/pyspark'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##df_json = sc1.read.json('/Users/RahulReddy/Desktop/pyspark/user.json',multiLine=True).printSchema()\n",
    "df_json = spark.read.option(\"multiline\",True).option(\"mode\",\"PERMISSIVE\")\\\n",
    ".json('/Users/RahulReddy/Desktop/pyspark/reviews_Patio_Lawn_and_Garden_5.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_json.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-----------+--------------+--------------------+-----------+--------------+\n",
      "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|    summary|unixReviewTime|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+--------------------+-----------+--------------+\n",
      "|B00002N674| [4, 4]|    4.0|Good USA company ...|06 21, 2011|A1JZFGZEZVWQPY|Carter H \"1amazon...|Great Hoses|    1308614400|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from test\").show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select reviewText from test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------------------+-----------+--------------+--------------------+-----------+--------------+\n",
      "|summary|      asin|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|    summary|unixReviewTime|\n",
      "+-------+----------+-------+--------------------+-----------+--------------+--------------------+-----------+--------------+\n",
      "|  count|         1|      1|                   1|          1|             1|                   1|          1|             1|\n",
      "|   mean|      null|    4.0|                null|       null|          null|                null|       null|   1.3086144E9|\n",
      "| stddev|      null|    NaN|                null|       null|          null|                null|       null|           NaN|\n",
      "|    min|B00002N674|    4.0|Good USA company ...|06 21, 2011|A1JZFGZEZVWQPY|Carter H \"1amazon...|Great Hoses|    1308614400|\n",
      "|    max|B00002N674|    4.0|Good USA company ...|06 21, 2011|A1JZFGZEZVWQPY|Carter H \"1amazon...|Great Hoses|    1308614400|\n",
      "+-------+----------+-------+--------------------+-----------+--------------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        reviewerName|\n",
      "+--------------------+\n",
      "|Carter H \"1amazon...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.select('reviewerName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(asin=u'B00002N674', helpful=[4, 4], overall=4.0, reviewText=u'Good USA company that stands behind their products. I have had to warranty two hoses and they send replacements right out to you. I had one burst after awhile, you could see it buldge for weeks before it went so no suprises. The other one was winter related as I am bad and leave them out most of the time. Highly reccomend. Note the hundred footer is heavy and like wresting an anaconda when its time to put away, but it does have a far reach.', reviewTime=u'06 21, 2011', reviewerID=u'A1JZFGZEZVWQPY', reviewerName=u'Carter H \"1amazonreviewer@gmail . com\"', summary=u'Great Hoses', unixReviewTime=1308614400)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1 = df_json.select('overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2 = df_1.distinct() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|overall|Multi|\n",
      "+-------+-----+\n",
      "|    4.0|  8.0|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_2.columns\n",
    "df_2.withColumn(\"Multi\",df_2['overall']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[overall: double]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.withColumnRenamed(\"Multi\",\"Lulti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 65256\r\n",
      "drwxr-xr-x  19 RahulReddy  staff       646 Jan  5 00:25 \u001b[34mlearningPySpark\u001b[m\u001b[m/\r\n",
      "-rw-r--r--   1 RahulReddy  staff  33396236 Jan  5 00:29 departuredelays.csv\r\n",
      "-rw-r--r--   1 RahulReddy  staff     11411 Jan  5 00:29 airport-codes-na.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr /Users/RahulReddy/Desktop/pyspark/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Users/RahulReddy/Desktop/pyspark/data/departuredelays.csv\",inferSchema=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'delay', 'distance', 'origin', 'destination']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38988"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((\"delay > 10\") and (\"destination = 'SFO'\")).count() \n",
    "#df.filter( (df['delay'] > 20) & (df['destination'] == 'SFO')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38988"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((\"destination = 'SFO'\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38988"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['destination'] == 'SFO').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"publicdata\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1 = spark.read.csv(\"/Users/RahulReddy/Desktop/pyspark/data/Police_Department_Incident_Reports__2018_to_Present.csv\"\\\n",
    "                      ,inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Incident Datetime',\n",
       " 'Incident Date',\n",
       " 'Incident Time',\n",
       " 'Incident Year',\n",
       " 'Incident Day of Week',\n",
       " 'Report Datetime',\n",
       " 'Row ID',\n",
       " 'Incident ID',\n",
       " 'Incident Number',\n",
       " 'CAD Number',\n",
       " 'Report Type Code',\n",
       " 'Report Type Description',\n",
       " 'Filed Online',\n",
       " 'Incident Code',\n",
       " 'Incident Category',\n",
       " 'Incident Subcategory',\n",
       " 'Incident Description',\n",
       " 'Resolution',\n",
       " 'Intersection',\n",
       " 'CNN',\n",
       " 'Police District',\n",
       " 'Analysis Neighborhood',\n",
       " 'Supervisor District',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'point']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     CNN|\n",
      "+--------+\n",
      "|30705000|\n",
      "|23250000|\n",
      "|23104000|\n",
      "|23104000|\n",
      "|23914000|\n",
      "|23914000|\n",
      "|23914000|\n",
      "|27247000|\n",
      "|21304000|\n",
      "|24319000|\n",
      "|22436000|\n",
      "|24933000|\n",
      "|33719000|\n",
      "|20356000|\n",
      "|25111000|\n",
      "|23827000|\n",
      "|23606000|\n",
      "|23603000|\n",
      "|24731000|\n",
      "|23876000|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.select(df_1['CNN']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Incident Datetime: string (nullable = true)\n",
      " |-- Incident Date: string (nullable = true)\n",
      " |-- Incident Time: string (nullable = true)\n",
      " |-- Incident Year: integer (nullable = true)\n",
      " |-- Incident Day of Week: string (nullable = true)\n",
      " |-- Report Datetime: string (nullable = true)\n",
      " |-- Row ID: long (nullable = true)\n",
      " |-- Incident ID: integer (nullable = true)\n",
      " |-- Incident Number: integer (nullable = true)\n",
      " |-- CAD Number: integer (nullable = true)\n",
      " |-- Report Type Code: string (nullable = true)\n",
      " |-- Report Type Description: string (nullable = true)\n",
      " |-- Filed Online: boolean (nullable = true)\n",
      " |-- Incident Code: integer (nullable = true)\n",
      " |-- Incident Category: string (nullable = true)\n",
      " |-- Incident Subcategory: string (nullable = true)\n",
      " |-- Incident Description: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Intersection: string (nullable = true)\n",
      " |-- CNN: decimal(8,0) (nullable = true)\n",
      " |-- Police District: string (nullable = true)\n",
      " |-- Analysis Neighborhood: string (nullable = true)\n",
      " |-- Supervisor District: integer (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%spark` not found.\n"
     ]
    }
   ],
   "source": [
    "%spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
